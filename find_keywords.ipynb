{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import spacy\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "from collections import Counter\n",
    "from util import preprocess_w2e\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['textcat', 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-defined word list\n",
    "with open('data/D_common.txt', 'r') as f:\n",
    "    D_common = set([line.strip() for line in f])\n",
    "        \n",
    "with open('data/D_tech.txt', 'r') as f:\n",
    "    D_tech = set([line.strip() for line in f])\n",
    "\n",
    "with open('data/D_whitelist.txt', 'r') as f:\n",
    "    D_whitelist = set([line.strip() for line in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(token_list):\n",
    "    word = [t for tl in token_list for t in tl]\n",
    "    counter = Counter(word)\n",
    "        \n",
    "    return counter, set(list(counter.keys()))\n",
    "\n",
    "def get_significat_words(word_list, n, current_c):\n",
    "    z = 1.645\n",
    "    word_list_ = []\n",
    "    for w in word_list:\n",
    "        f_w = current_c[w]\n",
    "        p_w = f_w/n\n",
    "        if p_w >= z*math.sqrt(p_w*(1-p_w)/n): \n",
    "            word_list_.append(w)\n",
    "    return word_list_\n",
    "\n",
    "def get_new_words(C, K, n, current_c):\n",
    "    new_words = set(K - C - D_common.union(D_tech))\n",
    "    new_words_ = get_significat_words(new_words, n, current_c)\n",
    "    print(f\"new words before: {len(new_words)}; new words after: {len(new_words_)}; \")\n",
    "    return new_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tweet_df = pd.read_json('../data/tweets_20220601_20220625.json')\n",
    "tweet_df = tweet_df.drop_duplicates(subset=['text'])\n",
    "\n",
    "tweets_past_list = []\n",
    "tweets_past_list.append(tweet_df.loc[tweet_df['created_at']>np.datetime64('2022-06-01')].loc[tweet_df['created_at']<=np.datetime64('2022-06-02')].reset_index(drop=True))\n",
    "tweets_past_list.append(tweet_df.loc[tweet_df['created_at']>np.datetime64('2022-06-02')].loc[tweet_df['created_at']<=np.datetime64('2022-06-03')].reset_index(drop=True))\n",
    "tweets_past_list.append(tweet_df.loc[tweet_df['created_at']>np.datetime64('2022-06-03')].loc[tweet_df['created_at']<=np.datetime64('2022-06-04')].reset_index(drop=True))\n",
    "tweets_cur = tweet_df.loc[tweet_df['created_at']>np.datetime64('2022-06-04')].loc[tweet_df['created_at']<=np.datetime64('2022-06-05')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_p is list of tokens \n",
    "tweets_past_list_p = [tl['text'].apply(lambda x: preprocess_w2e(x)) for tl in tweets_past_list]\n",
    "# tweets_past['text_p'] = tweets_past['text'].apply(lambda x: preprocess_w2e(x))\n",
    "tweets_cur['text_p'] = tweets_cur['text'].apply(lambda x: preprocess_w2e(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New keywords\n",
    "tokens_last  = tweets_past_list_p[-1].tolist()\n",
    "tokens_cur = tweets_cur.text_p.tolist()\n",
    "\n",
    "# C: last, t-1 ~ t\n",
    "# K: current, t\n",
    "last_c, C = count_tokens(tokens_last) # return (token, count)\n",
    "current_c, K = count_tokens(tokens_cur)\n",
    "n = len(tokens_last)\n",
    "\n",
    "new_words = get_new_words(C, K, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_all_df = pd.DataFrame()\n",
    "for i, text_p in enumerate(tweets_past_list_p):\n",
    "    past_c, _ = count_tokens(text_p) \n",
    "    past_df = pd.DataFrame.from_dict(past_c, orient='index').reset_index()\n",
    "    past_df = past_df.rename(columns={'index':'token', 0:'freq'})\n",
    "    past_df['ts'] = i \n",
    "    past_all_df = pd.concat([past_all_df, past_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-emerging keywords\n",
    "\n",
    "smoothing_f = 0.4\n",
    "reemerge_words = []\n",
    "k = len(tweets_past_list_p) # 3\n",
    "sf = (smoothing_f * (1 - (1 - smoothing_f)**(2 * k))) / (2 - smoothing_f)\n",
    "\n",
    "C_R = C.intersection(D_tech - D_whitelist)\n",
    "C_R_ = get_significat_words(C_R, n)\n",
    "\n",
    "print(past_all_df.shape)\n",
    "past_all_df = past_all_df[past_all_df['token'].isin(C_R_)]\n",
    "print(past_all_df.shape)\n",
    "\n",
    "for token, tmp_df in past_all_df.groupby('token'):\n",
    "    if tmp_df.shape[0] != 3:\n",
    "        for ts_ in set([0,1,2]) - set(tmp_df.ts.tolist()):\n",
    "            new_row = pd.DataFrame.from_dict([{'token':token, 'freq':0, 'ts':ts_}])\n",
    "            tmp_df = pd.concat([tmp_df, new_row])\n",
    "        tmp_df.sort_values(by=['ts'], ascending=[True])\n",
    "    tmp_df['EWMA'] = tmp_df['freq'].ewm(alpha=smoothing_f, adjust=False).mean()\n",
    "    fw = current_c[token]\n",
    "    fw_ = tmp_df.iloc[-1]['EWMA']\n",
    "    tmp_df['sigma'] = (tmp_df['freq'] - tmp_df['EWMA'])**2\n",
    "    if (fw-fw_)**2 >= 3.8 * tmp_df.sigma.sum()/k * sf:\n",
    "        reemerge_words.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reemerge_words), len(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reemerge_words.txt', 'w') as f:\n",
    "    for w in reemerge_words:\n",
    "        f.write(f\"{w}\\n\")\n",
    "\n",
    "with open('new_words.txt', 'w') as f:\n",
    "    for w in new_words:\n",
    "        f.write(f\"{w}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = set(reemerge_words).union(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_words = D_common.union(D_whitelist).union(K) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tweets = tweets_cur[tweets_cur['text_p'].apply(lambda x: len(set(x).intersection(keywords)) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fltr_tweets = filtered_tweets.text_p.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = np.zeros((len(fltr_tweets), len(fltr_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(fltr_tweets))): \n",
    "    for j in range(i, len(fltr_tweets)):\n",
    "        intersect = set(fltr_tweets[i]).intersection(set(fltr_tweets[j]))\n",
    "        union = set(fltr_tweets[i]).union(set(fltr_tweets[j]))\n",
    "        jaccard = len(intersect) / len(union) \n",
    "        sim_matrix[i][j] = jaccard\n",
    "        sim_matrix[j][i] = jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clusteirng\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clustering = AgglomerativeClustering(affinity='precomputed', linkage='complete').fit(sim_matrix)\n",
    "\n",
    "clustering.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = list(np.where(clustering.labels_==1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event1 = tweets_cur.iloc[sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering.n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event1['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweezers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a204d0fc7c9048c61aec0ffdb8371afea3c32a84afdb699b798daf26e300b9f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
